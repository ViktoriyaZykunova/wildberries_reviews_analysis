{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa7967b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_batch\n",
    "from transformers import pipeline\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc60a671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализация анализатора тональности\n",
    "analyzer = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"blanchefort/rubert-base-cased-sentiment\",\n",
    "    tokenizer=\"blanchefort/rubert-base-cased-sentiment\"\n",
    ")\n",
    "\n",
    "#Анализируем тональность текста с помощью модели\n",
    "def analyze_sentiment(text):\n",
    "    try:\n",
    "        result = analyzer(str(text))[0]\n",
    "        \n",
    "        # Преобразуем метки модели в стандартный формат\n",
    "        label_mapping = {\n",
    "            'positive': 'positive',\n",
    "            'neutral': 'neutral',\n",
    "            'negative': 'negative'\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            'sentiment_score': float(result['score']),\n",
    "            'sentiment_label': label_mapping.get(result['label'].lower(), 'neutral')\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при анализе текста: {str(e)}\")\n",
    "        return {'sentiment_score': 0.0, 'sentiment_label': 'neutral'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5736c58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Получаем отзывы из базы данных и устанавливаем лимит, чтобы не грузить базу\n",
    "def fetch_reviews(conn):\n",
    "    try:\n",
    "        query = \"\"\"\n",
    "            SELECT review_id, text \n",
    "            FROM reviews \n",
    "            WHERE review_id NOT IN (\n",
    "                SELECT review_id FROM sentiment_analysis\n",
    "            )\n",
    "            LIMIT 100 \n",
    "        \"\"\"\n",
    "        return pd.read_sql(query, conn)\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при загрузке отзывов: {str(e)}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f45745",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Сохраняем результаты анализа в базу данных\n",
    "def save_results(conn, results):\n",
    "    if not results:\n",
    "        return\n",
    "    \n",
    "    # Подготавливаем данные для вставки\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            data = [\n",
    "                (\n",
    "                    item['review_id'],\n",
    "                    item['sentiment_score'],\n",
    "                    item['sentiment_label'],\n",
    "                    datetime.now()\n",
    "                )\n",
    "                for item in results\n",
    "            ]\n",
    "            \n",
    "            # Вставляем данные пачками\n",
    "            execute_batch(cur,\n",
    "                \"\"\"\n",
    "                INSERT INTO sentiment_analysis \n",
    "                (review_id, sentiment_score, sentiment_label, created_at)\n",
    "                VALUES (%s, %s, %s, %s)\n",
    "                ON CONFLICT (review_id) DO UPDATE SET\n",
    "                    sentiment_score = EXCLUDED.sentiment_score,\n",
    "                    sentiment_label = EXCLUDED.sentiment_label,\n",
    "                    created_at = EXCLUDED.created_at\n",
    "                \"\"\",\n",
    "                data,\n",
    "                page_size=100\n",
    "            )\n",
    "            conn.commit()\n",
    "            print(f\"Сохранено {len(results)} результатов анализа\")\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"Ошибка при сохранении результатов: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea814c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Запускаем главную функцию\n",
    "def main():\n",
    "    print(\"Начало анализа тональности отзывов...\")\n",
    "    \n",
    "    try:\n",
    "        with psycopg2.connect(**DB_CONFIG) as conn:\n",
    "            #Получаем отзывы из БД\n",
    "            reviews_df = fetch_reviews(conn)\n",
    "            if reviews_df.empty:\n",
    "                print(\"Нет новых отзывов для анализа\")\n",
    "                return\n",
    "            \n",
    "            print(f\"Получено {len(reviews_df)} отзывов для анализа\")\n",
    "            \n",
    "            #Анализируем тональность с прогресс-баром, чтобы видеть процесс\n",
    "            results = []\n",
    "            for _, row in tqdm(reviews_df.iterrows(), total=len(reviews_df), desc=\"Анализ отзывов\"):\n",
    "                analysis = analyze_sentiment(row['text'])\n",
    "                results.append({\n",
    "                    'review_id': row['review_id'],\n",
    "                    **analysis\n",
    "                })\n",
    "            \n",
    "            #Сохраняем результаты в БД\n",
    "            save_results(conn, results)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка в основном процессе: {str(e)}\")\n",
    "    finally:\n",
    "        print(\"Анализ завершен\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2387f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "from psycopg2 import sql\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ed7c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Загружаем необходимые ресурсы NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#Проводим предварительную обработку текста\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    stop_words = set(stopwords.words('russian'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    tokens = [token for token in tokens if len(token) > 2]\n",
    "    \n",
    "    return tokens  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b603f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Извлекаем ключевые фразы из отзывов, топ-10 \n",
    "def extract_key_phrases(reviews, top_n=10):\n",
    "    all_tokens = []\n",
    "    for review in reviews:\n",
    "        all_tokens.extend(preprocess_text(review))\n",
    "    \n",
    "    if not all_tokens:\n",
    "        return []\n",
    "    \n",
    "    #Считаем частоту слов и биграмм\n",
    "    word_counts = Counter(all_tokens)\n",
    "    \n",
    "    #Извлекаем биграммы\n",
    "    bigrams = [f\"{all_tokens[i]} {all_tokens[i+1]}\" \n",
    "              for i in range(len(all_tokens)-1)]\n",
    "    bigram_counts = Counter(bigrams)\n",
    "    \n",
    "    #Объединяем результаты в Counter\n",
    "    combined = word_counts + bigram_counts\n",
    "    \n",
    "    #Получаем топ-10 фраз\n",
    "    return [phrase for phrase, _ in combined.most_common(top_n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09873f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Анализируем отзывы и сохраняем ключевын фразы\n",
    "def process_products(conn, limit: int = 100):\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            #Получаем продукты с отзывами\n",
    "            query = \"\"\"\n",
    "                SELECT r.product_id, array_agg(r.text) as reviews\n",
    "                FROM reviews r\n",
    "                WHERE r.text IS NOT NULL\n",
    "                GROUP BY r.product_id\n",
    "            \"\"\"\n",
    "            if limit:\n",
    "                query += f\" LIMIT {limit}\"\n",
    "                \n",
    "            cursor.execute(query)\n",
    "            products = cursor.fetchall()\n",
    "            \n",
    "            #Обрабатываем каждый продукт\n",
    "            for product_id, reviews in products:\n",
    "                phrases = extract_key_phrases(reviews)\n",
    "                if phrases:\n",
    "                    #Обновляем или добавляем записи в табличку\n",
    "                    cursor.execute(\"\"\"\n",
    "                        INSERT INTO key_phrases (product_id, phrases, created_at)\n",
    "                        VALUES (%s, %s, %s) \"\"\", (product_id, phrases, datetime.now()))\n",
    "            \n",
    "            conn.commit()\n",
    "            print(f\"Обработано продуктов: {len(products)}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"Ошибка: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55811ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Вызываем главную функцию\n",
    "def main():\n",
    "    conn = psycopg2.connect(**DB_CONFIG)\n",
    "    try:\n",
    "        process_products(conn, limit=100) #Устанавливаем лимит\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
