{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29a721e8",
   "metadata": {},
   "source": [
    "Поиск отзывов по сходству с запросом от пользователя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9dd458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290a1818",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_REVIEWS = 5000  #Берем выборку побольше для лучшего поиска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4340ff86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создаем класс для поиска отзывов\n",
    "class ReviewSearcher:\n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            max_features=10000,\n",
    "            ngram_range=(1, 2),\n",
    "            stop_words=stop_words\n",
    "        )\n",
    "        self.review_data = []\n",
    "        self.tfidf_matrix = None\n",
    "        \n",
    "    #Загружаем отзывы из базы данных для анализа\n",
    "    def load_data_from_db(self):\n",
    "        conn = psycopg2.connect(**DB_CONFIG)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        print(f\"Загрузка до {MAX_REVIEWS} отзывов из БД...\")\n",
    "        cursor.execute(f\"SELECT review_id, text FROM reviews LIMIT {MAX_REVIEWS}\")\n",
    "        self.review_data = cursor.fetchall()\n",
    "        \n",
    "        conn.close()\n",
    "        \n",
    "        if not self.review_data:\n",
    "            raise ValueError(\"В базе данных нет отзывов\")\n",
    "            \n",
    "        print(f\"Загружено {len(self.review_data)} отзывов (ограничение: {MAX_REVIEWS})\")\n",
    "\n",
    "    #Создаем поисковой индекс и проодим TF-IDF анализ\n",
    "    def prepare_search_index(self):\n",
    "        if len(self.review_data) > MAX_REVIEWS:\n",
    "            self.review_data = self.review_data[:MAX_REVIEWS]\n",
    "            print(f\"Ограничение количества отзывов до {MAX_REVIEWS}\")\n",
    "            \n",
    "        texts = [text for _, text in self.review_data]\n",
    "        print(\"Построение TF-IDF матрицы...\")\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(texts)\n",
    "\n",
    "    def find_similar_reviews(self, query_text, top_n=5):\n",
    "        \"\"\"Поиск похожих отзывов\"\"\"\n",
    "        if self.tfidf_matrix is None:\n",
    "            raise RuntimeError(\"Индекс не построен. Сначала вызовите prepare_search_index()\")\n",
    "            \n",
    "        #Векторизация запроса\n",
    "        query_vec = self.vectorizer.transform([query_text])\n",
    "        \n",
    "        #Вычисляем косинусовое сходство\n",
    "        similarities = cosine_similarity(query_vec, self.tfidf_matrix).flatten()\n",
    "        \n",
    "        #Группируем отзывы по диапазонам сходства\n",
    "        similarity_groups = defaultdict(list)\n",
    "        \n",
    "        for idx, similarity in enumerate(similarities):\n",
    "            similarity_rounded = round(similarity, 1)\n",
    "            review_id, text = self.review_data[idx]\n",
    "            similarity_groups[similarity_rounded].append({\n",
    "                'review_id': review_id,\n",
    "                'text': text,\n",
    "                'similarity': similarity\n",
    "            })\n",
    "        \n",
    "        #Сортируем группы по убыванию сходства\n",
    "        sorted_groups = sorted(similarity_groups.items(), key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "        #Формируем результат с топом отзывов из каждой группы\n",
    "        results = []\n",
    "        for similarity, reviews in sorted_groups:\n",
    "            reviews_sorted = sorted(reviews, key=lambda x: x['similarity'], reverse=True)\n",
    "            \n",
    "            group_info = {\n",
    "                'similarity_range': f\"{similarity:.1f}-{similarity+0.1:.1f}\",\n",
    "                'count': len(reviews),\n",
    "                'top_reviews': reviews_sorted[:top_n]\n",
    "            }\n",
    "            results.append(group_info)\n",
    "            \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4afb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Вызываем главную функцию\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        searcher = ReviewSearcher()\n",
    "        searcher.load_data_from_db()\n",
    "        searcher.prepare_search_index()\n",
    "        \n",
    "        while True:\n",
    "            print(\"\\nВведите текст для поиска похожих отзывов (или 'exit' для выхода):\") #Инпут для пользователя\n",
    "            query = input().strip()\n",
    "            \n",
    "            if query.lower() == 'exit': #Условие остановки анализа - введение слова exit\n",
    "                break\n",
    "                \n",
    "            if not query:\n",
    "                continue\n",
    "                \n",
    "            results = searcher.find_similar_reviews(query)\n",
    "            \n",
    "            print(f\"\\nРезультаты поиска для запроса: '{query}'\")\n",
    "            for group in results:\n",
    "                print(f\"\\nГруппа сходства {group['similarity_range']} - найдено отзывов: {group['count']}\")\n",
    "                print(f\"Топ-5 отзывов из этой группы:\")\n",
    "                for i, review in enumerate(group['top_reviews'], 1):\n",
    "                    print(f\"  {i}. ID: {review['review_id']}, Сходство: {review['similarity']:.4f}\")\n",
    "                    print(f\"     Текст: {review['text'][:100]}...\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb48ffb",
   "metadata": {},
   "source": [
    "Анализ эмоциональной окраски отзывов по категориям"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58a123f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создаем класс для эмоционального анализа\n",
    "class RussianEmotionAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.review_data = []\n",
    "        #Создаем структуру для хранения статистики\n",
    "        self.emotion_stats = {\n",
    "            'радость': {'count': 0, 'examples': []},\n",
    "            'грусть': {'count': 0, 'examples': []},\n",
    "            'ярость': {'count': 0, 'examples': []},\n",
    "            'нейтральный': {'count': 0, 'examples': []}\n",
    "        }\n",
    "        self.total_reviews = 0\n",
    "\n",
    "        \n",
    "        #Создаем словари эмоций \n",
    "        self.emotion_keywords = {\n",
    "            'радость': [\n",
    "                'отличн', 'прекрасн', 'рекоменд', 'довол', 'рад', 'восхит', \n",
    "                'супер', 'замечат', 'хорош', 'потряс', 'восторг', 'любим', \n",
    "                'удовольств', 'счаст', 'благодар', 'восхищен', 'превосходн', 'шикарн'\n",
    "            ],\n",
    "            'грусть': [\n",
    "                'плох', 'груст', 'разочар', 'жал', 'печал', 'неудов', \n",
    "                'сожален', 'тоск', 'скорб', 'несчаст', 'обид', 'тяжел', \n",
    "                'печаль', 'скучн', 'уныл', 'неприятн', 'отказ'\n",
    "            ],\n",
    "            'ярость': [\n",
    "                'ужас', 'кошмар', 'злит', 'бесит', 'возмущ', 'отврат', \n",
    "                'ненавист', 'гнев', 'бешен', 'раздраж', 'мерзк', 'противн', \n",
    "                'худш', 'гадост', 'отвратительн', 'неприятн'\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    #Загрузка отзывов из базы данных, начиная с первого review_id (была проблема - брал для анализа записи не из анчала таблицы)\n",
    "    def load_data_from_db(self, limit=10000):\n",
    "        \n",
    "        conn = psycopg2.connect(**DB_CONFIG)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        #Получаем минимальный review_id\n",
    "        cursor.execute(\"SELECT MIN(review_id) FROM reviews\")\n",
    "        min_id = cursor.fetchone()[0]\n",
    "        \n",
    "        print(f\"Загрузка отзывов из БД начиная с ID {min_id} (лимит: {limit})...\")\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT review_id, text \n",
    "            FROM reviews \n",
    "            WHERE review_id >= %s\n",
    "            ORDER BY review_id\n",
    "            LIMIT %s\n",
    "        \"\"\", (min_id, limit))\n",
    "        \n",
    "        self.review_data = cursor.fetchall()\n",
    "        conn.close()\n",
    "        \n",
    "        if not self.review_data:\n",
    "            raise ValueError(\"В базе данных нет отзывов\")\n",
    "            \n",
    "        print(f\"Загружено {len(self.review_data)} отзывов (первый ID: {self.review_data[0][0]}, последний ID: {self.review_data[-1][0]})\")\n",
    "\n",
    "    #Анализ эмоциональной окраски текста\n",
    "    def analyze_emotion(self, text):\n",
    "        \n",
    "        text = text.lower()\n",
    "        \n",
    "        for emotion, keywords in self.emotion_keywords.items():\n",
    "            if any(keyword in text for keyword in keywords):\n",
    "                return emotion\n",
    "        \n",
    "        return 'нейтральный'\n",
    "\n",
    "    #Обрабатываем отзывовы\n",
    "    def process_reviews(self):\n",
    "        print(\"Анализ эмоциональной окраски отзывов...\")\n",
    "        \n",
    "        for review_id, text in self.review_data:\n",
    "            try:\n",
    "                emotion = self.analyze_emotion(text)\n",
    "                \n",
    "                #Обновляем счетчик\n",
    "                self.emotion_stats[emotion]['count'] += 1\n",
    "                self.total_reviews += 1\n",
    "                \n",
    "                #Сохраняем примеры (ограничение - не более 5)\n",
    "                if len(self.emotion_stats[emotion]['examples']) < 5:\n",
    "                    self.emotion_stats[emotion]['examples'].append({\n",
    "                        'id': review_id,\n",
    "                        'text': text[:200] + '...' if len(text) > 200 else text #Ограничиваем вывод текста отзыва, если он слишком большой\n",
    "                    })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Ошибка при анализе отзыва {review_id}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    #Выводим результаты\n",
    "    def print_results(self):\n",
    "        print(\"\\nРезультаты анализа эмоциональной окраски отзывов:\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Обработано отзывов: {self.total_reviews}\")\n",
    "        print(f\"Диапазон ID: {self.review_data[0][0]} - {self.review_data[-1][0]}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        for emotion, stats in self.emotion_stats.items():\n",
    "            percentage = (stats['count'] / self.total_reviews * 100) if self.total_reviews > 0 else 0 #Рассчитываем процент от общего числа обработанных отзывов\n",
    "            \n",
    "            print(f\"\\n{emotion.upper():^60}\")\n",
    "            print(\"-\" * 60)\n",
    "            print(f\"Количество: {stats['count']} ({percentage:.1f}%)\") #Выводим количество и процент по каждой группе\n",
    "            \n",
    "            #Выводим примеры по каждой группе\n",
    "            if stats['examples']:\n",
    "                print(\"\\nПримеры отзывов:\")\n",
    "                for i, example in enumerate(stats['examples'], 1):\n",
    "                    print(f\"{i}. ID: {example['id']}\")\n",
    "                    print(f\"   Текст: {example['text']}\\n\")\n",
    "            \n",
    "            print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b83b352",
   "metadata": {},
   "source": [
    "Поиск аномалий в тексте (через питон)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d417b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef8aeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создаем класс для анализа выявления аномалий\n",
    "class AnomalyDetector:\n",
    "    def __init__(self):\n",
    "        self.review_data = []\n",
    "        self.anomalies = defaultdict(list)\n",
    "        self.user_stats = defaultdict(dict)\n",
    "        \n",
    "        # Параметры для обнаружения аномалий\n",
    "        self.params = {\n",
    "            'max_caps_ratio': 0.3, #Максимальная доля заглавных букв\n",
    "            'max_links': 1, #Максимальное количество ссылок\n",
    "            'max_special_chars': 20, #Максимальное количество спецсимволов\n",
    "            'min_unique_words': 3, #Минимальное количество уникальных слов\n",
    "            'max_reviews_per_user': 3, #Максимальное количество отзывов от пользователя\n",
    "            'min_similarity_ratio': 0.8 #Порог схожести отзывов\n",
    "        }\n",
    "\n",
    "    #Загрузка отзывов из базы данных с ограничением\n",
    "    def load_data_from_db(self, limit=500):\n",
    "\n",
    "        conn = psycopg2.connect(**DB_CONFIG)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        print(f\"Загрузка отзывов из БД (лимит: {limit})...\")\n",
    "        query = \"\"\"\n",
    "        SELECT r.review_id, r.text, r.mark, r.user_id\n",
    "        FROM reviews r\n",
    "        LIMIT %s\n",
    "        \"\"\"\n",
    "        cursor.execute(query, (limit,))\n",
    "        self.review_data = cursor.fetchall()\n",
    "        \n",
    "        conn.close()\n",
    "        \n",
    "        if not self.review_data:\n",
    "            raise ValueError(\"В базе данных нет отзывов\")\n",
    "            \n",
    "        print(f\"Загружено {len(self.review_data)} отзывов\")\n",
    "\n",
    "    #Вычисление коэффициента схожести двух текстов отзывов\n",
    "    def calculate_similarity(self, text1, text2):\n",
    "        return SequenceMatcher(None, text1.lower(), text2.lower()).ratio()\n",
    "\n",
    "    #Функция обнаружение аномальных отзывов\n",
    "    def detect_anomalies(self):\n",
    "        print(\"Поиск аномальных отзывов...\")\n",
    "        \n",
    "        user_reviews = defaultdict(list)\n",
    "        \n",
    "        #Собираем отзывы по пользователям\n",
    "        for review in self.review_data:\n",
    "            review_id, text, mark, user_id = review\n",
    "            user_reviews[user_id].append((review_id, text, mark))\n",
    "        \n",
    "        #Анализируем каждого пользователя\n",
    "        for user_id, reviews in user_reviews.items():\n",
    "            total_reviews = len(reviews)\n",
    "            \n",
    "            #Сохраняем статистику по пользователю\n",
    "            self.user_stats[user_id] = {\n",
    "                'total_reviews': total_reviews,\n",
    "                'similar_reviews': set()\n",
    "            }\n",
    "            \n",
    "            #Прводим проверку на схожесть отзывов\n",
    "            for i in range(len(reviews)):\n",
    "                for j in range(i+1, len(reviews)):\n",
    "                    review_id1, text1, mark1 = reviews[i]\n",
    "                    review_id2, text2, mark2 = reviews[j]\n",
    "                    \n",
    "                    similarity = self.calculate_similarity(text1, text2)\n",
    "                    if similarity >= self.params['min_similarity_ratio']:\n",
    "                        self.user_stats[user_id]['similar_reviews'].update([review_id1, review_id2])\n",
    "                        self._add_anomaly(\n",
    "                            review_id1,\n",
    "                            text1,\n",
    "                            mark1,\n",
    "                            user_id,\n",
    "                            f\"Найден похожий отзыв (схожесть: {similarity:.1%}, ID: {review_id2})\" #Показываем процент схожести\n",
    "                        )\n",
    "                        self._add_anomaly(\n",
    "                            review_id2,\n",
    "                            text2,\n",
    "                            mark2,\n",
    "                            user_id,\n",
    "                            f\"Найден похожий отзыв (схожесть: {similarity:.1%}, ID: {review_id1})\" \n",
    "                        )\n",
    "            \n",
    "            #Проверка на количество отзывов (добавляем только для одного отзыва пользователя, так как 1 пользователь мог оставить несколько сходих комментариев, и нам не надо, чтобы вывод дублировался каждый раз)\n",
    "            if total_reviews > self.params['max_reviews_per_user']:\n",
    "                first_review_id = reviews[0][0]\n",
    "                self._add_anomaly(\n",
    "                    first_review_id,\n",
    "                    reviews[0][1],\n",
    "                    reviews[0][2],\n",
    "                    user_id,\n",
    "                    f\"Пользователь оставил {total_reviews} отзывов (показан 1 из них)\"\n",
    "                )\n",
    "        \n",
    "        #Проверка других аномалий для всех отзывов\n",
    "        for review in self.review_data:\n",
    "            review_id, text, mark, user_id = review\n",
    "            #Пропускаем отзывы, которые уже добавлены как \"похожие\" или \"много отзывов\"\n",
    "            if review_id in self.anomalies:\n",
    "                continue\n",
    "            self._check_basic_anomalies(review_id, text, mark, user_id)\n",
    "\n",
    "    #Добавляем аномалии в результаты\n",
    "    def _add_anomaly(self, review_id, text, mark, user_id, message):\n",
    "        if review_id in self.anomalies:\n",
    "            if isinstance(message, list):\n",
    "                self.anomalies[review_id]['anomalies'].extend(message)\n",
    "            else:\n",
    "                self.anomalies[review_id]['anomalies'].append(message)\n",
    "        else:\n",
    "            self.anomalies[review_id] = {\n",
    "                'text': text[:200] + '...' if len(text) > 200 else text,  #Опять ограничиваем текст отзыва\n",
    "                'mark': mark,\n",
    "                'user_id': user_id,\n",
    "                'anomalies': [message] if not isinstance(message, list) else message\n",
    "            }\n",
    "\n",
    "    #Проверка базовых аномалий в отзыве\n",
    "    def _check_basic_anomalies(self, review_id, text, mark, user_id):\n",
    "        anomalies_found = []\n",
    "        \n",
    "        #Проверка на капс\n",
    "        caps_count = sum(1 for c in text if c.isupper())\n",
    "        caps_ratio = caps_count / len(text) if len(text) > 0 else 0\n",
    "        if caps_ratio > self.params['max_caps_ratio']:\n",
    "            anomalies_found.append(f\"Много заглавных букв ({caps_ratio:.0%})\")\n",
    "        \n",
    "        #Проверка на ссылки\n",
    "        link_count = len(re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text))\n",
    "        if link_count > self.params['max_links']:\n",
    "            anomalies_found.append(f\"Обнаружены ссылки ({link_count})\")\n",
    "        \n",
    "        #Проверка спецсимволов\n",
    "        special_chars = len(re.findall(r'[^\\w\\s]', text))\n",
    "        if special_chars > self.params['max_special_chars']:\n",
    "            anomalies_found.append(f\"Много спецсимволов ({special_chars})\")\n",
    "        \n",
    "        #Проверка уникальных слов\n",
    "        words = re.findall(r'[а-яА-ЯёЁ]+', text.lower())\n",
    "        unique_words = len(set(words))\n",
    "        if unique_words < self.params['min_unique_words']:\n",
    "            anomalies_found.append(f\"Мало уникальных слов ({unique_words})\")\n",
    "        \n",
    "        if anomalies_found:\n",
    "            self._add_anomaly(review_id, text, mark, user_id, anomalies_found)\n",
    "\n",
    "    #Выводим результаты\n",
    "    def print_results(self):\n",
    "        print(f\"\\nНайдено {len(self.anomalies)} аномальных отзывов:\")\n",
    "        print(\"=\" * 100)\n",
    "        \n",
    "        for review_id, data in self.anomalies.items():\n",
    "            print(f\"\\nID отзыва: {review_id}\")\n",
    "            print(f\"Пользователь: {data['user_id']}\")\n",
    "            print(f\"Оценка: {data['mark']}/5\")\n",
    "            print(f\"Текст: {data['text']}\")\n",
    "            print(\"\\nОбнаруженные аномалии:\")\n",
    "            for anomaly in data['anomalies']:\n",
    "                print(f\"- {anomaly}\")\n",
    "            \n",
    "            print(\"-\" * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4bc079",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Главная функция запуска анализа\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        detector = AnomalyDetector()\n",
    "        detector.load_data_from_db(limit=500)\n",
    "        detector.detect_anomalies()\n",
    "        detector.print_results()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
